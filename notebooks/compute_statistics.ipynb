{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute statistical informations: mean, std and percentiles : \n",
    "\n",
    "This notebook computes the mean, std and needed percentiles of waiting time for the arrival time of each transport type, hour and station. It uses the historical data of the SBB. It will write the created data as an ORC file in the home of the user running the notebook.\n",
    "\n",
    "Note, this notebook takes approximately 30 minutes to run.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": \"dslab-group_final\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from geopy.distance import distance as geo_distance\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "from networkx.algorithms.shortest_paths.weighted import dijkstra_path\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stops in ZÃ¼rich area\n",
    "stops_zurich = spark.read.format('orc').load(\"/user/gottraux/nodes.orc\")\\\n",
    "                                        .select('stop_id').distinct().rdd.flatMap(lambda x:x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from find_train_type_correspondace.ipynb, to transform `verkehrsmittel_text` in the same format\n",
    "replace_actual = {\n",
    "    'BUS': 'Bus', # Buses\n",
    "    'B': 'Bus',\n",
    "    'NFB': 'Bus',\n",
    "    'KB': 'Bus',\n",
    "    'BAT': 'Bus',\n",
    "    'Trm': 'Tram', # Trams\n",
    "    'T': 'Tram',\n",
    "    'TRAM': 'Tram',\n",
    "    'ATZ': 'ARZ', #AutoZug\n",
    "    'D': 'RE', # Regional\n",
    "    'RB': 'R',\n",
    "    'M': 'Metro', # Metro\n",
    "    'ICE': 'IC', # InterCityExpress, but routes.txt doesn't have that category\n",
    "    'IRE': 'IR', # InterRegioExpress, but routes.txt doesn't have that category\n",
    "    'BN': '', # Night\n",
    "    'TN': '',\n",
    "    'SN': '',\n",
    "    'BT': '',\n",
    "    'VAE': '', # Panorama trains in the Alps\n",
    "    'PE': '',\n",
    "    'TER': '', # France\n",
    "    'TE2': '',\n",
    "    'RJX': '', # International\n",
    "    'null': '', # Other\n",
    "    '': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"string\")\n",
    "def replace_verkehrsmittel_text(text):\n",
    "    if text in replace_actual.keys():\n",
    "        return replace_actual[text]\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = spark.read.format('orc').load('/data/sbb/orc/istdaten/')\\\n",
    "                                 .where((col(\"ankunftszeit\") != \"\") & (col(\"an_prognose\") != \"\"))\\\n",
    "                                 .select(col('bpuic').alias('stop_id'), # Transform bpuic\n",
    "                                         replace_verkehrsmittel_text(col('verkehrsmittel_text')).alias('verkehrsmittel_text'), # Translate `verkehrsmittel_text`\n",
    "                                         from_unixtime(unix_timestamp('ankunftszeit', 'dd.MM.yyy HH:mm')).alias('ankunftszeit'),\n",
    "                                         from_unixtime(unix_timestamp('an_prognose', 'dd.MM.yyy HH:mm:ss')).alias('an_prognose'),\n",
    "                                         col('an_prognose_status'))\\\n",
    "                                 .where(col('stop_id').isin(stops_zurich))\\\n",
    "                                 .where(col(\"verkehrsmittel_text\") != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = (actual.withColumn('hour', hour(col('ankunftszeit')))\n",
    "               .withColumn('diff', unix_timestamp('an_prognose') - unix_timestamp('ankunftszeit')) # Compute delay\n",
    "               .select(col('stop_id'), col('verkehrsmittel_text'), col('an_prognose_status'), col('hour'), col('diff')))\n",
    "\n",
    "# Take only trains at reasonable hours\n",
    "actual = actual.where((col('hour') >= 8) & (col('hour') <= 20))\n",
    "\n",
    "# Count how many 'REAL' an_prognose status there are, if there are >= 10 we say that there are enough of them\n",
    "enough_real_values = actual.where(col('an_prognose_status') == \"REAL\")\\\n",
    "                        .groupBy('stop_id', 'verkehrsmittel_text', 'hour')\\\n",
    "                        .agg(count('diff').alias('count'))\\\n",
    "                        .withColumn(\"enough_values\", col(\"count\") >= 10)\\\n",
    "                        .select(col('stop_id').alias('stop_id2'), \n",
    "                                col('verkehrsmittel_text').alias('verkehrsmittel_text2'), \n",
    "                                col('hour').alias('hour2'), col('enough_values'))\n",
    "\n",
    "# Left join with enough_values\n",
    "actual = actual.join(enough_real_values, (actual.stop_id == enough_real_values.stop_id2) &\\\n",
    "                                        (actual.verkehrsmittel_text == enough_real_values.verkehrsmittel_text2) &\\\n",
    "                                        (actual.hour == enough_real_values.hour2), \"left\")\\\n",
    "                    .select('stop_id', 'verkehrsmittel_text', 'an_prognose_status', 'hour', 'diff', 'enough_values')\n",
    "\n",
    "# Set `enough_values` to False for trains that didn't have any 'REAL' `an_prognose_status`\n",
    "actual = actual.na.fill(False)\n",
    "\n",
    "# Take only REAL an_prognose_status if there are enough values\n",
    "# If there aren't enough we use all the data available\n",
    "actual = actual.where((~(col(\"enough_values\"))) | (col(\"an_prognose_status\") == \"REAL\"))\n",
    "\n",
    "# Compute mean and std\n",
    "actual = actual.groupBy('stop_id', 'verkehrsmittel_text', 'hour')\\\n",
    "                .agg(mean(\"diff\").alias(\"mean\"), stddev(\"diff\").alias(\"std\"), count(\"diff\").alias('number_of_records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"float\")\n",
    "def compute_delay_percentile(mean, std, percentile):\n",
    "    X = norm(loc=mean, scale=std)\n",
    "    return float(X.ppf(percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now compute and add all the percentiles 90-99\n",
    "# That means, given the delay distribution how much time do we have to wait to be 'percentile'% sure we'll be arrived by then\n",
    "# This will be useful when validating paths to only take safe edges\n",
    "actual = actual.withColumn('p_90', compute_delay_percentile(col('mean'), col('std'), lit(0.9)))\\\n",
    "                .withColumn('p_91', compute_delay_percentile(col('mean'), col('std'), lit(0.91)))\\\n",
    "                .withColumn('p_92', compute_delay_percentile(col('mean'), col('std'), lit(0.92)))\\\n",
    "                .withColumn('p_93', compute_delay_percentile(col('mean'), col('std'), lit(0.93)))\\\n",
    "                .withColumn('p_94', compute_delay_percentile(col('mean'), col('std'), lit(0.94)))\\\n",
    "                .withColumn('p_95', compute_delay_percentile(col('mean'), col('std'), lit(0.95)))\\\n",
    "                .withColumn('p_96', compute_delay_percentile(col('mean'), col('std'), lit(0.96)))\\\n",
    "                .withColumn('p_97', compute_delay_percentile(col('mean'), col('std'), lit(0.97)))\\\n",
    "                .withColumn('p_98', compute_delay_percentile(col('mean'), col('std'), lit(0.98)))\\\n",
    "                .withColumn('p_99', compute_delay_percentile(col('mean'), col('std'), lit(0.99)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "username = os.environ['JUPYTERHUB_USER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual.write.format(\"orc\").mode('overwrite').save(\"/user/{}/delay_distribution_percentiles.orc\".format(username))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
